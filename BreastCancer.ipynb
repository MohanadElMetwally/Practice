{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "cancer.keys() #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.DataFrame(data = cancer.data,columns= cancer.feature_names) #type: ignore\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "0                  0.2654          0.4601                  0.11890      0  \n",
       "1                  0.1860          0.2750                  0.08902      0  \n",
       "2                  0.2430          0.3613                  0.08758      0  \n",
       "3                  0.2575          0.6638                  0.17300      0  \n",
       "4                  0.1625          0.2364                  0.07678      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = cancer.target #type: ignore\n",
    "data = pd.DataFrame(data=cancer.data, columns=cancer.feature_names) #type: ignore\n",
    "data['class'] = target\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, target, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        42\n",
      "           1       0.99      0.99      0.99        72\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "y_predict = lr.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x24984a83750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, y_predict)\n",
    "ConfusionMatrixDisplay(matrix, display_labels=cancer.target_names).plot() #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00200009, 0.00200009, 0.00200033, 0.00099969, 0.00200105]), 'score_time': array([0.0010004 , 0.        , 0.        , 0.00099993, 0.        ]), 'test_score': array([0.95604396, 0.97802198, 0.96703297, 0.98901099, 0.98901099])}\n",
      "[0.95604396 0.97802198 0.96703297 0.98901099 0.98901099]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "scores = cross_validate(lr, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00199771, 0.00300026, 0.00100064, 0.00299931, 0.00300145,\n",
       "        0.00300026, 0.00200009, 0.00200081, 0.00300097, 0.00300121,\n",
       "        0.0030005 , 0.00200033, 0.00200033, 0.00099897, 0.00200033,\n",
       "        0.00300121, 0.00300026, 0.00200033, 0.00200057, 0.00200057,\n",
       "        0.00200057, 0.00300074, 0.00200057, 0.00099993, 0.00200057,\n",
       "        0.00300097, 0.00099969, 0.00299978, 0.00300074, 0.00200105,\n",
       "        0.00300026, 0.00300074, 0.00300097, 0.00200033, 0.00100017,\n",
       "        0.00200009, 0.00199986, 0.00100017, 0.00300002, 0.00199986,\n",
       "        0.00200033, 0.0030005 , 0.0030005 , 0.00300145, 0.00199938,\n",
       "        0.00200009, 0.00199986, 0.00200057, 0.0030005 , 0.0010004 ,\n",
       "        0.0030005 , 0.00300026, 0.00200105, 0.00300026, 0.00200057,\n",
       "        0.00099993, 0.00300074, 0.00200009, 0.00200057, 0.00200057,\n",
       "        0.00200033, 0.00300097, 0.00300097, 0.00300026, 0.00200033,\n",
       "        0.00300097, 0.00300026, 0.00300074, 0.00300074, 0.0030005 ,\n",
       "        0.00300097, 0.0030005 , 0.00200081, 0.0010004 , 0.0010004 ,\n",
       "        0.00200033, 0.00300074, 0.00300121, 0.00200033, 0.00300097,\n",
       "        0.00300074, 0.00200057, 0.00199652, 0.0030005 , 0.00300074,\n",
       "        0.00200057, 0.0030005 , 0.00200081, 0.00300026, 0.00200057,\n",
       "        0.00200057, 0.00200057, 0.00200033, 0.00099993, 0.00200033,\n",
       "        0.0030005 , 0.00200081, 0.00200057, 0.0030005 , 0.00099993,\n",
       "        0.0010004 , 0.00200009, 0.00200033, 0.00300097, 0.00200033,\n",
       "        0.00300074, 0.00300097, 0.0030005 , 0.00200057, 0.00300074,\n",
       "        0.00199938, 0.0030005 , 0.00300002, 0.00200081, 0.0030005 ,\n",
       "        0.0030005 , 0.0030005 , 0.00200105, 0.00200057, 0.00199986,\n",
       "        0.0030005 , 0.00200081, 0.00200057, 0.00199986, 0.00200129,\n",
       "        0.00199986, 0.00200009, 0.00199986, 0.00200009, 0.0030005 ,\n",
       "        0.00199986, 0.00200009, 0.00200033, 0.00200057, 0.00199986,\n",
       "        0.00200057, 0.00300026, 0.00200057, 0.0030005 , 0.00300074,\n",
       "        0.00200033, 0.0010004 , 0.00200009, 0.00200009, 0.00300074,\n",
       "        0.0010004 , 0.00200033, 0.00200057, 0.00200033, 0.00200033,\n",
       "        0.00199986, 0.00200081, 0.00300026, 0.00300121, 0.00200033,\n",
       "        0.00200105, 0.0030005 , 0.00300074, 0.00300097, 0.00300074,\n",
       "        0.00300026, 0.00300074, 0.00200081, 0.00100088, 0.00200057,\n",
       "        0.00200081, 0.00300097, 0.00200033, 0.0030005 , 0.0030005 ,\n",
       "        0.00200081, 0.00200057, 0.00200057, 0.0010004 , 0.00200057,\n",
       "        0.00199986, 0.00200105, 0.00200129, 0.00199986, 0.00300002,\n",
       "        0.00200081, 0.00200081, 0.00300026, 0.00300193, 0.00199914,\n",
       "        0.00199986, 0.00200057, 0.00200057, 0.00200105, 0.00200057,\n",
       "        0.00100017, 0.00100017, 0.00198817, 0.00299978, 0.0030005 ,\n",
       "        0.00300097, 0.00300145, 0.00299954, 0.00200129, 0.00100017,\n",
       "        0.00299883, 0.00300097, 0.00300074, 0.00199962, 0.00299954,\n",
       "        0.00307393, 0.00200391, 0.00199986, 0.00200033, 0.00200033,\n",
       "        0.00200009, 0.00300097, 0.00199962, 0.00300026, 0.00300145,\n",
       "        0.002002  , 0.00200009, 0.00300097, 0.0030005 , 0.00300121,\n",
       "        0.00300026, 0.00200057, 0.00200105, 0.00199986, 0.00200033,\n",
       "        0.00200033, 0.00300097, 0.00300145, 0.00199962, 0.002002  ,\n",
       "        0.00299954, 0.00200009, 0.00300026, 0.0030005 , 0.0030005 ,\n",
       "        0.00200033, 0.00200009, 0.00199986, 0.00200057, 0.00199986,\n",
       "        0.00200009, 0.00200081, 0.00199986, 0.00200009, 0.00200033,\n",
       "        0.00300026, 0.00200057, 0.00199986, 0.0030005 , 0.00200081,\n",
       "        0.00199986, 0.00199986, 0.00200081, 0.00199986, 0.00200009,\n",
       "        0.00100017, 0.00199986, 0.00200057, 0.00200033, 0.00199986,\n",
       "        0.0030005 , 0.00299978, 0.00300097, 0.00100017, 0.00200009,\n",
       "        0.00199986, 0.00300026, 0.00300074, 0.00200009, 0.00200081,\n",
       "        0.0010004 , 0.00200009, 0.00200009, 0.00200081, 0.00100017,\n",
       "        0.00200009, 0.00300002, 0.00200081, 0.00199986, 0.00199986,\n",
       "        0.00300074, 0.0030005 , 0.0030005 , 0.00200009, 0.00199986,\n",
       "        0.00200033, 0.00199986, 0.00300097, 0.00200033, 0.00100017,\n",
       "        0.00200009, 0.00300026, 0.00300097, 0.00199986, 0.00200057,\n",
       "        0.0010004 , 0.00300002, 0.00200081, 0.00199986, 0.00199938,\n",
       "        0.00100064, 0.0010004 , 0.00200033, 0.00199986, 0.00343251,\n",
       "        0.00150394, 0.0030005 , 0.00200081, 0.00200057, 0.00200009,\n",
       "        0.0030005 , 0.00200009, 0.00300074, 0.00200009, 0.00199914,\n",
       "        0.00300026, 0.0030005 , 0.00200009, 0.00200033, 0.0030005 ,\n",
       "        0.00300026, 0.00300026, 0.00199962, 0.00200105, 0.00300026,\n",
       "        0.00200081, 0.00300026, 0.00200057, 0.00200009, 0.00200009,\n",
       "        0.00200009, 0.00300074, 0.0030005 , 0.00200081, 0.00200009,\n",
       "        0.00300026, 0.00300074, 0.00199986, 0.00100017, 0.00199986,\n",
       "        0.00200009, 0.00200009, 0.00200009, 0.00200057, 0.00300074,\n",
       "        0.00200057, 0.00100017, 0.00200391, 0.00200057, 0.00299954,\n",
       "        0.00200081, 0.00200057, 0.00300026, 0.00200057, 0.00200176,\n",
       "        0.00200033, 0.00300097, 0.00300169, 0.00200081, 0.00200057,\n",
       "        0.00200033, 0.00200057, 0.00299883, 0.00352621, 0.00433087,\n",
       "        0.00200033, 0.00300097, 0.00200009, 0.00200057, 0.0030005 ,\n",
       "        0.00300074, 0.00300121, 0.00300074, 0.00200081, 0.0030005 ,\n",
       "        0.00300121, 0.0030005 , 0.00200033, 0.00200009, 0.00299883,\n",
       "        0.00300097, 0.00300145, 0.00099945, 0.00200009, 0.0030005 ,\n",
       "        0.00200129, 0.00100064, 0.00200033, 0.00300002, 0.00200081,\n",
       "        0.00200009, 0.00200033, 0.00200081, 0.00200105, 0.00300026,\n",
       "        0.0030005 , 0.00300074, 0.00300074, 0.00300074, 0.0010004 ,\n",
       "        0.00300002, 0.00300074, 0.00199986, 0.00200009, 0.00300074,\n",
       "        0.00200105, 0.00300026, 0.0030005 , 0.0010004 , 0.00300241,\n",
       "        0.00200009, 0.00200057, 0.00300097, 0.00200057, 0.00200105,\n",
       "        0.00200081, 0.002002  , 0.0030005 , 0.00300074, 0.00200057,\n",
       "        0.00200009, 0.00200057, 0.00300074, 0.00300074, 0.0030005 ,\n",
       "        0.00200105, 0.00300145, 0.00300002, 0.00200033, 0.00200081,\n",
       "        0.00200105, 0.00199962, 0.00300097, 0.00299883, 0.00400138,\n",
       "        0.00201011, 0.00300026, 0.00300121, 0.00300074, 0.00199986,\n",
       "        0.00200033, 0.00200152, 0.00200009, 0.00300074, 0.00200081,\n",
       "        0.00200033, 0.00199986, 0.00200057, 0.00200009, 0.00200033,\n",
       "        0.00200057, 0.00300169, 0.00199986, 0.00099969, 0.00200033]),\n",
       " 'score_time': array([0.00100112, 0.        , 0.00100136, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00099993, 0.        , 0.        ,\n",
       "        0.00100017, 0.00100017, 0.        , 0.00100064, 0.        ,\n",
       "        0.        , 0.00100017, 0.        , 0.        , 0.00099993,\n",
       "        0.        , 0.        , 0.        , 0.00100017, 0.        ,\n",
       "        0.        , 0.0010004 , 0.        , 0.        , 0.00099993,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00099993,\n",
       "        0.00100088, 0.00100064, 0.00100017, 0.        , 0.0010004 ,\n",
       "        0.        , 0.        , 0.00100064, 0.        , 0.0010004 ,\n",
       "        0.        , 0.        , 0.00099993, 0.0010004 , 0.0010004 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0010004 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00099993,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0010004 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00099969, 0.0010004 , 0.00099993,\n",
       "        0.00100017, 0.        , 0.00099993, 0.00099993, 0.        ,\n",
       "        0.00100017, 0.0010047 , 0.00099969, 0.00100017, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00100017, 0.        , 0.00100064, 0.0010004 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00100017, 0.        , 0.        , 0.        , 0.00099993,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00100112, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00100017, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00099993,\n",
       "        0.00100064, 0.        , 0.00100064, 0.        , 0.00100088,\n",
       "        0.        , 0.00100064, 0.0010004 , 0.        , 0.00100064,\n",
       "        0.        , 0.        , 0.00100017, 0.        , 0.        ,\n",
       "        0.00100064, 0.00099993, 0.00100064, 0.        , 0.        ,\n",
       "        0.00099993, 0.0010004 , 0.00100017, 0.00100017, 0.00100064,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00100017,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00100017, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00100064, 0.0010004 ,\n",
       "        0.        , 0.00099993, 0.        , 0.        , 0.        ,\n",
       "        0.00100017, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00099921, 0.00099993, 0.00100017, 0.        , 0.        ,\n",
       "        0.00100017, 0.0010128 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00100112,\n",
       "        0.        , 0.00100064, 0.        , 0.00100112, 0.        ,\n",
       "        0.        , 0.00100064, 0.0010004 , 0.00100088, 0.00099993,\n",
       "        0.        , 0.        , 0.00100017, 0.        , 0.        ,\n",
       "        0.00099897, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00099993, 0.        , 0.        , 0.00100017,\n",
       "        0.        , 0.        , 0.00099993, 0.00100017, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00100064,\n",
       "        0.0010004 , 0.        , 0.00100064, 0.        , 0.00100064,\n",
       "        0.        , 0.0010004 , 0.00100064, 0.0010004 , 0.0010004 ,\n",
       "        0.        , 0.00100088, 0.0010004 , 0.        , 0.        ,\n",
       "        0.00100064, 0.        , 0.00100064, 0.00100064, 0.0010004 ,\n",
       "        0.00100064, 0.0010004 , 0.        , 0.00100064, 0.        ,\n",
       "        0.00100088, 0.        , 0.00100064, 0.00100017, 0.00100088,\n",
       "        0.00100064, 0.        , 0.00100064, 0.        , 0.        ,\n",
       "        0.        , 0.0010004 , 0.        , 0.0010004 , 0.00100017,\n",
       "        0.00100064, 0.        , 0.00100064, 0.00100088, 0.        ,\n",
       "        0.        , 0.        , 0.00100088, 0.00100064, 0.00100064,\n",
       "        0.0010004 , 0.        , 0.0010004 , 0.00100064, 0.00100017,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00100017,\n",
       "        0.00100017, 0.        , 0.        , 0.00100112, 0.0010004 ,\n",
       "        0.00099993, 0.00100017, 0.0010004 , 0.        , 0.        ,\n",
       "        0.0010004 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0010004 , 0.        , 0.        , 0.0010016 , 0.00100064,\n",
       "        0.        , 0.00100064, 0.0010004 , 0.        , 0.00100088,\n",
       "        0.        , 0.00100112, 0.        , 0.        , 0.        ,\n",
       "        0.0010004 , 0.        , 0.0010004 , 0.        , 0.0010004 ,\n",
       "        0.        , 0.        , 0.        , 0.0010004 , 0.00100064,\n",
       "        0.        , 0.        , 0.        , 0.00100017, 0.        ,\n",
       "        0.0010004 , 0.0010016 , 0.        , 0.        , 0.00099993,\n",
       "        0.        , 0.00100017, 0.        , 0.        , 0.        ,\n",
       "        0.00099993, 0.0010004 , 0.        , 0.00099993, 0.00099921,\n",
       "        0.00100017, 0.        , 0.        , 0.00099969, 0.00100017,\n",
       "        0.        , 0.00100255, 0.        , 0.        , 0.        ,\n",
       "        0.00100017, 0.        , 0.        , 0.0010004 , 0.        ,\n",
       "        0.        , 0.        , 0.00099945, 0.        , 0.00099993,\n",
       "        0.        , 0.00099993, 0.0010004 , 0.00100231, 0.        ,\n",
       "        0.        , 0.0010004 , 0.00100064, 0.        , 0.00099993,\n",
       "        0.00099945, 0.00099969, 0.0010004 , 0.        , 0.00100017,\n",
       "        0.0010004 , 0.        , 0.        , 0.00099993, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00100017,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00099993, 0.        , 0.00100064, 0.00100017, 0.        ,\n",
       "        0.0010004 , 0.00100017, 0.        , 0.        , 0.        ,\n",
       "        0.00099969, 0.0009985 , 0.        , 0.        , 0.00100088,\n",
       "        0.00099969, 0.00100017, 0.        , 0.        , 0.        ,\n",
       "        0.00099969, 0.        , 0.0010004 , 0.        , 0.00100017,\n",
       "        0.        , 0.00100088, 0.0010016 , 0.        , 0.00100017,\n",
       "        0.00099421, 0.00100136, 0.00099945, 0.0010004 , 0.00099993,\n",
       "        0.        , 0.        , 0.00099969, 0.        , 0.00099993,\n",
       "        0.00100064, 0.        , 0.00100088, 0.        , 0.00100017,\n",
       "        0.00100017, 0.        , 0.        , 0.        , 0.        ]),\n",
       " 'test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_validate(lr, X_train, y_train, cv=loo, scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.97362637 0.96923077        nan 0.97802198 0.98021978 0.98021978\n",
      " 0.97362637 0.96923077        nan 0.97802198 0.98021978 0.98021978\n",
      " 0.97582418 0.97582418        nan 0.98021978 0.98021978 0.98021978\n",
      " 0.97582418 0.97362637        nan 0.98021978 0.98021978 0.98021978\n",
      " 0.96043956 0.98021978        nan 0.96923077 0.98021978 0.96923077\n",
      " 0.96043956 0.97142857        nan 0.96923077 0.97142857 0.96923077]\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohanad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;max_iter&#x27;: [100, 1000],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;max_iter&#x27;: [100, 1000],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.1, 1, 10], 'max_iter': [100, 1000],\n",
       "                         'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['liblinear', 'saga', 'lbfgs']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga', 'lbfgs'], 'max_iter': [100, 1000]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid=params, cv= 5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, solver='saga'),\n",
       " {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'},\n",
       " 0.9802197802197803)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        42\n",
      "           1       0.97      0.99      0.98        72\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "[[40  2]\n",
      " [ 1 71]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = grid.predict(X_test) # type: ignore\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
